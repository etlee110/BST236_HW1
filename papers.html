<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Latest Papers from arXiv!!!!</title>
    <link rel="stylesheet" href="styles/main.css">
</head>
<body>
    <header>
        <h1>READ THE LATEST PAPERS (Times are UTC)</h1>
        <nav>
            <ul>
                <li><a href="index.html">Home HERE</a></li>
                <li><a href="pacman.html">Play Pac-Man HERE</a></li>
                <li><a href="papers.html">Latest Papers HERE</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section id="papers">
            <h2>Recent Papers HURRY</h2>
            <p>Search keyword: physics</p>
            <p>Last updated: 2025-05-02 01:21:17</p>
            <div id="paper-list">
                <!-- Papers will be dynamically inserted here -->

        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2504.21856v1" target="_blank">Holography and Causality in the Karch-Randall Braneworld</a></h3>
            <p><strong>Authors:</strong> Hao Geng, Lisa Randall</p>
            <p>It has been argued that the existence of a geodesic shortcut in the Karch-Randall (KR) braneworld rules out the possibility for a low-energy intermediate theory, even when gravity is turned off. We study this problem in an explicit example with two symmetrically placed KR branes. We find that there can be a consistent quantization of bulk matter fields with a holographic intermediate description. In our model, there is no causality violation. We use this example to study potential implications of the bulk geodesic shortcut. We find that there are possible enhancements of the correlation or entanglement between the two branes that is directly correlated with the extra-dimensional geometry. This could affect the low-energy regime when there are interactions between low and high energy modes. Our model makes it clear that any possible causality violation can only arise from the UV. Independently of the quantization, an intermediate description should be valid as an EFT.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2405.20774v3" target="_blank">Can We Trust Embodied Agents? Exploring Backdoor Attacks against   Embodied LLM-based Decision-Making Systems</a></h3>
            <p><strong>Authors:</strong> Ruochen Jiao, Shaoyuan Xie, Justin Yue, Takami Sato, Lixu Wang, Yixuan Wang, Qi Alfred Chen, Qi Zhu</p>
            <p>Large Language Models (LLMs) have shown significant promise in real-world decision-making tasks for embodied artificial intelligence, especially when fine-tuned to leverage their inherent common sense and reasoning abilities while being tailored to specific applications. However, this fine-tuning process introduces considerable safety and security vulnerabilities, especially in safety-critical cyber-physical systems. In this work, we propose the first comprehensive framework for Backdoor Attacks against LLM-based Decision-making systems (BALD) in embodied AI, systematically exploring the attack surfaces and trigger mechanisms. Specifically, we propose three distinct attack mechanisms: word injection, scenario manipulation, and knowledge injection, targeting various components in the LLM-based decision-making pipeline. We perform extensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in autonomous driving and home robot tasks, demonstrating the effectiveness and stealthiness of our backdoor triggers across various attack channels, with cases like vehicles accelerating toward obstacles and robots placing knives on beds. Our word and knowledge injection attacks achieve nearly 100% success rate across multiple models and datasets while requiring only limited access to the system. Our scenario manipulation attack yields success rates exceeding 65%, reaching up to 90%, and does not require any runtime system intrusion. We also assess the robustness of these attacks against defenses, revealing their resilience. Our findings highlight critical security vulnerabilities in embodied LLM systems and emphasize the urgent need for safeguarding these systems to mitigate potential risks.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2504.21855v1" target="_blank">ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D   Physics Modeling for Complex Motion and Interaction</a></h3>
            <p><strong>Authors:</strong> Qihao Liu, Ju He, Qihang Yu, Liang-Chieh Chen, Alan Yuille</p>
            <p>In recent years, video generation has seen significant advancements. However, challenges still persist in generating complex motions and interactions. To address these challenges, we introduce ReVision, a plug-and-play framework that explicitly integrates parameterized 3D physical knowledge into a pretrained conditional video generation model, significantly enhancing its ability to generate high-quality videos with complex motion and interactions. Specifically, ReVision consists of three stages. First, a video diffusion model is used to generate a coarse video. Next, we extract a set of 2D and 3D features from the coarse video to construct a 3D object-centric representation, which is then refined by our proposed parameterized physical prior model to produce an accurate 3D motion sequence. Finally, this refined motion sequence is fed back into the same video diffusion model as additional conditioning, enabling the generation of motion-consistent videos, even in scenarios involving complex actions and interactions. We validate the effectiveness of our approach on Stable Video Diffusion, where ReVision significantly improves motion fidelity and coherence. Remarkably, with only 1.5B parameters, it even outperforms a state-of-the-art video generation model with over 13B parameters on complex video generation by a substantial margin. Our results suggest that, by incorporating 3D physical knowledge, even a relatively small video diffusion model can generate complex motions and interactions with greater realism and controllability, offering a promising solution for physically plausible video generation.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2504.21854v2" target="_blank">SPARO: Surface-code Pauli-based Architectural Resource Optimization for   Fault-tolerant Quantum Computing</a></h3>
            <p><strong>Authors:</strong> Shuwen Kan, Zefan Du, Chenxu Liu, Meng Wang, Yufei Ding, Ang Li, Ying Mao, Samuel Stein</p>
            <p>Surface codes represent a leading approach for quantum error correction (QEC), offering a path towards universal fault-tolerant quantum computing (FTQC). However, efficiently implementing algorithms, particularly using Pauli-based computation (PBC) with lattice surgery, necessitates careful resource optimization. Prior work often employs static layouts and simplified error models. These typically fail to capture the full costs and dynamic nature of active computation, leading to resource bottlenecks and suboptimal architectural designs. To address this, we introduce SPARO. SPARO features a comprehensive logical error model based on a large corpus of numerical simulations encompassing active Pauli-based computation (PBC) operations-including Pauli product measurements (PPMs), idling qubits, and patch rotations. Our numerical models are integrated within an end-to-end compilation pipeline. SPARO analyzes algorithm-specific bottlenecks arising from constraints such as limited routing areas or magic-state factory throughput. SPARO then dynamically allocates available hardware resources, balancing compute, routing, and magic-state distillation, to minimize space-time overhead and logical error rates for specific workloads. Our simulations demonstrate that SPARO effectively identifies critical resource trade-offs. When evaluated on benchmark circuits, SPARO identifies resource configurations achieving up to 51.11% logical error rate reductions for 433-qubit ADDER circuits when compared to state-of-the-art static layouts using an identical total resource budget. This dynamic approach enables effective co-optimization of PBC execution and surface-code architectures, significantly improving overall resource efficiency. SPARO will be open sourced.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2504.21853v1" target="_blank">A Survey of Interactive Generative Video</a></h3>
            <p><strong>Authors:</strong> Jiwen Yu, Yiran Qin, Haoxuan Che, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Hao Chen, Xihui Liu</p>
            <p>Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2504.21852v1" target="_blank">Polka-dotted Stars: a Hierarchical Model for Mapping Stellar Surfaces   Using Occultation Light Curves and the Case of TOI-3884</a></h3>
            <p><strong>Authors:</strong> Sabina Sagynbayeva, Will M. Farr, Brett Morris, Rodrigo Luger</p>
            <p>We present StarryStarryProcess, a novel hierarchical Bayesian framework for mapping stellar surfaces using exoplanet transit light curves. While previous methods relied solely on stellar rotational light curves--which contain limited information about spot properties -- our approach leverages planetary transits as probes of stellar surfaces. When a planet crosses a spot during transit, it creates a distinctive change in the light curve that directly reveals spot properties. Our model integrates planetary transit modeling with stellar variability analysis by combining the spherical harmonic surface map representation from starry, the probabilistic approach to spot properties of StarryProcess, and a comprehensive transit model that accounts for spot-crossing events during transits. We demonstrate through synthetic data experiments that our model successfully recovers spot distributions, stellar orientation, and spot physical properties. We extend the framework to handle evolving stellar surfaces through time-dependent modeling. Applying our method to TESS observations of TOI-3884, we find evidence for high-latitude spot concentrations and significant spin-orbit misalignment. The transit-based approach overcomes fundamental limitations of previous models by providing constraints on spot properties that would remain hidden in the null space of rotational light curves alone. This methodology enables more accurate exoplanet characterization by disentangling stellar activity due to starspots from planetary signals while simultaneously providing insights into stellar magnetic activity patterns. The whole paper is reproducible, and can be found by clicking the GitHub icon.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2504.21847v1" target="_blank">Differentiable Room Acoustic Rendering with Multi-View Vision Priors</a></h3>
            <p><strong>Authors:</strong> Derong Jin, Ruohan Gao</p>
            <p>An immersive acoustic experience enabled by spatial audio is just as crucial as the visual aspect in creating realistic virtual environments. However, existing methods for room impulse response estimation rely either on data-demanding learning-based models or computationally expensive physics-based modeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic Rendering (AV-DAR), a framework that leverages visual cues extracted from multi-view images and acoustic beam tracing for physics-based room acoustic rendering. Experiments across six real-world environments from two datasets demonstrate that our multimodal, physics-based approach is efficient, interpretable, and accurate, significantly outperforming a series of prior methods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves comparable performance to models trained on 10 times more data while delivering relative gains ranging from 16.6% to 50.9% when trained at the same scale.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2504.21846v1" target="_blank">Active Light Modulation to Counter Manipulation of Speech Visual Content</a></h3>
            <p><strong>Authors:</strong> Hadleigh Schwartz, Xiaofeng Yan, Charles J. Carver, Xia Zhou</p>
            <p>High-profile speech videos are prime targets for falsification, owing to their accessibility and influence. This work proposes Spotlight, a low-overhead and unobtrusive system for protecting live speech videos from visual falsification of speaker identity and lip and facial motion. Unlike predominant falsification detection methods operating in the digital domain, Spotlight creates dynamic physical signatures at the event site and embeds them into all video recordings via imperceptible modulated light. These physical signatures encode semantically-meaningful features unique to the speech event, including the speaker's identity and facial motion, and are cryptographically-secured to prevent spoofing. The signatures can be extracted from any video downstream and validated against the portrayed speech content to check its integrity. Key elements of Spotlight include (1) a framework for generating extremely compact (i.e., 150-bit), pose-invariant speech video features, based on locality-sensitive hashing; and (2) an optical modulation scheme that embeds >200 bps into video while remaining imperceptible both in video and live. Prototype experiments on extensive video datasets show Spotlight achieves AUCs $\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified videos. Further, Spotlight is highly robust across recording conditions, video post-processing techniques, and white-box adversarial attacks on its video feature extraction methodologies.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2504.21845v1" target="_blank">On the Efficacy of the Peeling Decoder for the Quantum Expander Code</a></h3>
            <p><strong>Authors:</strong> Jefrin Sharmitha Prabhu, Abhinav Vaishya, Shobhit Bhatnagar, Aryaman Manish Kolhe, V. Lalitha, P. Vijay Kumar</p>
            <p>The problem of recovering from qubit erasures has recently gained attention as erasures occur in many physical systems such as photonic systems, trapped ions, superconducting qubits and circuit quantum electrodynamics. While several linear-time decoders for error correction are known, their error-correcting capability is limited to half the minimum distance of the code, whereas erasure correction allows one to go beyond this limit. As in the classical case, stopping sets pose a major challenge in designing efficient erasure decoders for quantum LDPC codes. In this paper, we show through simulation, that an attractive alternative here, is the use of quantum expander codes in conjunction with the peeling decoder that has linear complexity. We also discuss additional techniques including small-set-flip decoding, that can be applied following the peeling operation, to improve decoding performance and their associated complexity.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2504.21844v1" target="_blank">Scalable Multi-Task Learning for Particle Collision Event Reconstruction   with Heterogeneous Graph Neural Networks</a></h3>
            <p><strong>Authors:</strong> William Sutcliffe, Marta Calvi, Simone Capelli, Jonas Eschle, Julián García Pardiñas, Abhijit Mathad, Azusa Uzuki, Nicola Serra</p>
            <p>The growing luminosity frontier at the Large Hadron Collider is challenging the reconstruction and analysis of particle collision events. Increased particle multiplicities are straining latency and storage requirements at the data acquisition stage, while new complications are emerging, including higher background levels and more frequent particle vertex misassociations. This in turn necessitates the development of more holistic and scalable reconstruction methods that take advantage of recent advances in machine learning. We propose a novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique representations for diverse particle collision relationships and integrated graph pruning layers for scalability. Trained with a multi-task paradigm in an environment mimicking the LHCb experiment, this HGNN significantly improves beauty hadron reconstruction performance. Notably, it concurrently performs particle vertex association and graph pruning within a single framework. We quantify reconstruction and pruning performance, demonstrate enhanced inference time scaling with event complexity, and mitigate potential performance loss using a weighted message passing scheme.</p>
        </div>
        <hr>
        
<!-- END PAPERS -->
            </div>
        </section>
    </main>
</body>
</html>
