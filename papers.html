<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Latest Papers from arXiv!!!!</title>
    <link rel="stylesheet" href="styles/main.css">
</head>
<body>
    <header>
        <h1>READ THE LATEST PAPERS (Times are UTC)</h1>
        <nav>
            <ul>
                <li><a href="index.html">Home HERE</a></li>
                <li><a href="pacman.html">Play Pac-Man HERE</a></li>
                <li><a href="papers.html">Latest Papers HERE</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section id="papers">
            <h2>Recent Papers HURRY</h2>
            <p>Search keyword: physics</p>
            <p>Last updated: 2025-02-27 01:14:10</p>
            <div id="paper-list">
                <!-- Papers will be dynamically inserted here -->

        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2405.05255v2" target="_blank">Diffusion-HMC: Parameter Inference with Diffusion-model-driven   Hamiltonian Monte Carlo</a></h3>
            <p><strong>Authors:</strong> Nayantara Mudur, Carolina Cuesta-Lazaro, Douglas P. Finkbeiner</p>
            <p>Diffusion generative models have excelled at diverse image generation and reconstruction tasks across fields. A less explored avenue is their application to discriminative tasks involving regression or classification problems. The cornerstone of modern cosmology is the ability to generate predictions for observed astrophysical fields from theory and constrain physical models from observations using these predictions. This work uses a single diffusion generative model to address these interlinked objectives -- as a surrogate model or emulator for cold dark matter density fields conditional on input cosmological parameters, and as a parameter inference model that solves the inverse problem of constraining the cosmological parameters of an input field. The model is able to emulate fields with summary statistics consistent with those of the simulated target distribution. We then leverage the approximate likelihood of the diffusion generative model to derive tight constraints on cosmology by using the Hamiltonian Monte Carlo method to sample the posterior on cosmological parameters for a given test image. Finally, we demonstrate that this parameter inference approach is more robust to small perturbations of noise to the field than baseline parameter inference networks.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2502.18462v1" target="_blank">Scalable Equilibrium Sampling with Sequential Boltzmann Generators</a></h3>
            <p><strong>Authors:</strong> Charlie B. Tan, Avishek Joey Bose, Chen Lin, Leon Klein, Michael M. Bronstein, Alexander Tong</p>
            <p>Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann generators tackle this problem by pairing powerful normalizing flows with importance sampling to obtain statistically independent samples under the target distribution. In this paper, we extend the Boltzmann generator framework and introduce Sequential Boltzmann generators (SBG) with two key improvements. The first is a highly efficient non-equivariant Transformer-based normalizing flow operating directly on all-atom Cartesian coordinates. In contrast to equivariant continuous flows of prior methods, we leverage exactly invertible non-equivariant architectures which are highly efficient both during sample generation and likelihood computation. As a result, this unlocks more sophisticated inference strategies beyond standard importance sampling. More precisely, as a second key improvement we perform inference-time scaling of flow samples using annealed Langevin dynamics which transports samples toward the target distribution leading to lower variance (annealed) importance weights which enable higher fidelity resampling with sequential Monte Carlo. SBG achieves state-of-the-art performance w.r.t. all metrics on molecular systems, demonstrating the first equilibrium sampling in Cartesian coordinates of tri, tetra, and hexapeptides that were so far intractable for prior Boltzmann generators.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2502.18459v1" target="_blank">Spectral modelling of Cygnus A between 110 and 250 MHz. Impact on the   LOFAR 21-cm signal power spectrum</a></h3>
            <p><strong>Authors:</strong> E. Ceccotti, A. R. Offringa, L. V. E. Koopmans, F. G. Mertens, M. Mevius, A. Acharya, S. A. Brackenhoff, B. Ciardi, B. K. Gehlot, R. Ghara, J. K. Chege, S. Ghosh, C. Höfer, I. Hothi, I. T. Iliev, J. P. McKean, S. Munshi, S. Zaroubi</p>
            <p>Studying the redshifted 21-cm signal from the the neutral hydrogen during the Epoch of Reionization and Cosmic Dawn is fundamental for understanding the physics of the early universe. One of the challenges that 21-cm experiments face is the contamination by bright foreground sources, such as Cygnus A, for which accurate spatial and spectral models are needed to minimise the residual contamination after their removal. In this work, we develop a new, high-resolution model of Cygnus A using Low Frequency Array (LOFAR) observations in the $110{-}250$ MHz range, improving upon previous models by incorporating physical spectral information through the forced-spectrum method during multi-frequency deconvolution. This approach addresses the limitations of earlier models by providing a more accurate representation of the complex structure and spectral behaviour of Cygnus A, including the spectral turnover in its brightest hotspots. The impact of this new model on the LOFAR 21-cm signal power spectrum is assessed by comparing it with both simulated and observed North Celestial Pole data sets. Significant improvements are observed in the cylindrical power spectrum along the Cygnus A direction, highlighting the importance of having spectrally accurate models of the brightest foreground sources. However, this improvement is washed out in the spherical power spectrum, where we measure differences of a few hundred mK at $k<0.63\,h\,\text{cMpc}^{-1}$, but not statistically significant. The results suggest that other systematic effects must be mitigated before a substantial impact on 21-cm power spectrum can be achieved.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2404.09955v2" target="_blank">Effects of Superradiance in Active Galactic Nuclei</a></h3>
            <p><strong>Authors:</strong> Priyanka Sarmah, Himanshu Verma, Kingman Cheung, Joseph Silk</p>
            <p>A supermassive black hole (SMBH) at the core of an active galactic nucleus (AGN) provides room for the elusive ultra-light scalar particles (ULSP) to be produced through a phenomenon called \textit{superradiance}. This phenomenon produces a cloud of scalar particles around the black hole by draining its spin angular momentum. In this work, we present a study of the superradiant instability due to a scalar field in the vicinity of the central SMBH in an AGN. We begin by showing that the time-evolution of the gravitational coupling $\alpha$ in a realistic ambiance created by the accretion disk around the SMBH in AGN leads to interesting consequences such as the amplified growth of the scalar cloud, enhancement of the gravitational wave emission rate, and appearance of higher modes of superradiance within the age of the Universe. We then explore the consequence of superradiance on the characteristics of the AGN. Using the Novikov-Thorne model for an accretion disk, we divide the full spectrum into three wavelength bands- X-ray ($10^{-4}-10^{-2}~\mu$m), UV (0.010-0.4~$\mu$m), and Vis-IR (0.4-100~$\mu$m) and observe sudden drops in the time-variations of the luminosities across these bands and Eddington ratio ($f_{\textrm{Edd}}$) with a characteristic timescale of superradiance. Using a uniform distribution of spin and mass of the SMBHs in AGNs, we demonstrate the appearance of depleted regions and accumulations along the boundaries of these regions in the planes of different band-luminosities and $f_{\textrm{Edd}}$. Finally, we discuss some possible signatures of superradiance that can be drawn from the observed time-variation of the AGN luminosities.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2502.18457v1" target="_blank">Towards a Composite Framework for Simultaneous Exploration of New   Physics in Background and Perturbed Universes</a></h3>
            <p><strong>Authors:</strong> Shibendu Gupta Choudhury, Purba Mukherjee, Anjan Ananda Sen</p>
            <p>We investigate deviations from $\Lambda$CDM by independently parameterizing modifications in the background evolution and the growth of structures. The background is characterized by two parameters, $A$ and $B$, which reduce to $\Omega_{m0}$ and $2/3$ in the $\Lambda$CDM limit, while deviations in the growth of structures are captured through a new fitting function for $f\sigma_8$ involving the growth index $\gamma$. Using recent observational datasets, we find significant evidence for departures from $\Lambda$CDM, with data suggesting a preference for modifications in the background evolution, likely due to evolving dark energy, rather than modifications to gravity. Additionally, our framework alleviates the $H_0$ tension and addresses the $\sigma_8$ and $S_8$ tensions. We also demonstrate that future high-precision RSD data could unveil correlations between background and perturbation parameters that are currently suppressed by observational uncertainties, offering deeper insights into cosmic evolution.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2502.13889v3" target="_blank">On the Addressability Problem on CSS Codes</a></h3>
            <p><strong>Authors:</strong> Jérôme Guyot, Samuel Jaques</p>
            <p>Recent discoveries in asymptotically good quantum codes have intensified research on their application in quantum computation and fault-tolerant operations. This study focuses on the addressability problem within CSS codes: what circuits might implement logical gates on strict subsets of logical qubits? With some notion of fault-tolerance, we show some impossibility results: for CSS codes with non-zero rate, one cannot address a logical H, HP, PH, nor CNOT to any non-empty strict subset of logical qubits using a circuit made only from 1-local Clifford gates.   Furthermore, we show that one cannot permute the logical qubits in a code purely by permuting the physical qubits, if the rate of the code is (asymptotically) greater than 1/3 and the distance is at least 3. We can show a similar no-go result for CNOTs and CZs between two such high-rate codes, albeit with a less reasonable restriction to circuits that we call ``global'' (though recent addressable CCZ gates use global circuits).   This work pioneers the study of distance-preserving addressability in quantum codes, mainly by considering automorphisms of the code. This perspective offers new insights and potential directions for future research. We argue that studying this trade off between addressability and efficiency of the codes is essential to understand better how to do efficient quantum computation.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2402.13984v3" target="_blank">Stability-Aware Training of Machine Learning Force Fields with   Differentiable Boltzmann Estimators</a></h3>
            <p><strong>Authors:</strong> Sanjeev Raja, Ishan Amin, Fabian Pedregosa, Aditi S. Krishnapriyan</p>
            <p>Machine learning force fields (MLFFs) are an attractive alternative to ab-initio methods for molecular dynamics (MD) simulations. However, they can produce unstable simulations, limiting their ability to model phenomena occurring over longer timescales and compromising the quality of estimated observables. To address these challenges, we present Stability-Aware Boltzmann Estimator (StABlE) Training, a multi-modal training procedure which leverages joint supervision from reference quantum-mechanical calculations and system observables. StABlE Training iteratively runs many MD simulations in parallel to seek out unstable regions, and corrects the instabilities via supervision with a reference observable. We achieve efficient end-to-end automatic differentiation through MD simulations using our Boltzmann Estimator, a generalization of implicit differentiation techniques to a broader class of stochastic algorithms. Unlike existing techniques based on active learning, our approach requires no additional ab-initio energy and forces calculations to correct instabilities. We demonstrate our methodology across organic molecules, tetrapeptides, and condensed phase systems, using three modern MLFF architectures. StABlE-trained models achieve significant improvements in simulation stability, data efficiency, and agreement with reference observables. The stability improvements cannot be matched by reducing the simulation timestep; thus, StABlE Training effectively allows for larger timesteps. By incorporating observables into the training process alongside first-principles calculations, StABlE Training can be viewed as a general semi-empirical framework applicable across MLFF architectures and systems. This makes it a powerful tool for training stable and accurate MLFFs, particularly in the absence of large reference datasets. Our code is available at https://github.com/ASK-Berkeley/StABlE-Training.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2502.18453v1" target="_blank">Shift orbifolds, decompactification limits, and lattices</a></h3>
            <p><strong>Authors:</strong> Dan Israel, Ilarion V. Melnikov, Yann Proto</p>
            <p>We describe the general shift orbifold of a Narain CFT and use this to investigate decompactification limits in the heterotic Narain moduli space. We also comment on higher rank theories and describe some applications to the CFT based on the Leech lattice and its shift orbifolds.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2502.18452v1" target="_blank">FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in   Object-Based Common Sense Reasoning for Disaster Response</a></h3>
            <p><strong>Authors:</strong> Mollie Shichman, Claire Bonial, Austin Blodgett, Taylor Hudson, Francis Ferraro, Rachel Rudinger</p>
            <p>Large Language Models (LLMs) have the potential for substantial common sense reasoning. However, these capabilities are often emergent in larger models. This means smaller models that can be run locally are less helpful and capable with respect to certain reasoning tasks. To meet our problem space requirements, we fine-tune smaller LLMs to disaster domains, as these domains involve complex and low-frequency physical common sense knowledge. We introduce a pipeline to create Field Ready Instruction Decoding Agent (FRIDA) models, where domain experts and linguists combine their knowledge to make high-quality seed data that is used to generate synthetic data for fine-tuning. We create a set of 130 seed instructions for synthetic generation, a synthetic dataset of 25000 instructions, and 119 evaluation instructions relating to both general and earthquake-specific object affordances. We fine-tune several LLaMa and Mistral instruction-tuned models and find that FRIDA models outperform their base models at a variety of sizes. We then run an ablation study to understand which kinds of synthetic data most affect performance and find that training physical state and object function common sense knowledge alone improves over FRIDA models trained on all data. We conclude that the FRIDA pipeline is capable of instilling general common sense, but needs to be augmented with information retrieval for specific domain knowledge.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2410.05898v6" target="_blank">Manifolds, Random Matrices and Spectral Gaps: The geometric phases of   generative diffusion</a></h3>
            <p><strong>Authors:</strong> Enrico Ventura, Beatrice Achilli, Gianluigi Silvestri, Carlo Lucibello, Luca Ambrogioni</p>
            <p>In this paper, we investigate the latent geometry of generative diffusion models under the manifold hypothesis. For this purpose, we analyze the spectrum of eigenvalues (and singular values) of the Jacobian of the score function, whose discontinuities (gaps) reveal the presence and dimensionality of distinct sub-manifolds. Using a statistical physics approach, we derive the spectral distributions and formulas for the spectral gaps under several distributional assumptions, and we compare these theoretical predictions with the spectra estimated from trained networks. Our analysis reveals the existence of three distinct qualitative phases during the generative process: a trivial phase; a manifold coverage phase where the diffusion process fits the distribution internal to the manifold; a consolidation phase where the score becomes orthogonal to the manifold and all particles are projected on the support of the data. This `division of labor' between different timescales provides an elegant explanation of why generative diffusion models are not affected by the manifold overfitting phenomenon that plagues likelihood-based models, since the internal distribution and the manifold geometry are produced at different time points during generation.</p>
        </div>
        <hr>
        
<!-- END PAPERS -->
            </div>
        </section>
    </main>
</body>
</html>
