<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Latest Papers from arXiv!!!!</title>
    <link rel="stylesheet" href="styles/main.css">
</head>
<body>
    <header>
        <h1>READ THE LATEST PAPERS (Times are UTC)</h1>
        <nav>
            <ul>
                <li><a href="index.html">Home HERE</a></li>
                <li><a href="pacman.html">Play Pac-Man HERE</a></li>
                <li><a href="papers.html">Latest Papers HERE</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section id="papers">
            <h2>Recent Papers HURRY</h2>
            <p>Search keyword: physics</p>
            <p>Last updated: 2025-03-21 01:17:21</p>
            <div id="paper-list">
                <!-- Papers will be dynamically inserted here -->

        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2503.15483v1" target="_blank">Emergent coding phases and hardware-tailored quantum codes</a></h3>
            <p><strong>Authors:</strong> Gaurav Gyawali, Henry Shackleton, Zhu-Xi Luo, Michael Lawler</p>
            <p>Finding good quantum codes for a particular hardware application and determining their error thresholds is central to quantum error correction. The threshold defines the noise level where a quantum code switches between a coding and a no-coding \emph{phase}. Provided sufficiently frequent error correction, the quantum capacity theorem guarantees the existence of an optimal code that provides a maximum communication rate. By viewing a system experiencing repeated error correction as a novel form of matter, this optimal code, in analogy to Jaynes's maximum entropy principle of quantum statistical mechanics, \emph{defines a phase}. We explore coding phases from this perspective using the Open Random Unitary Model (ORUM), which is a quantum circuit with depolarizing and dephasing channels. Using numerical optimization, we find this model hosts three phases: a maximally mixed phase, a ``$Z_2$ code'' that breaks its U(1) gauge symmetry down to $Z_2$, and a no-coding phase with first-order transitions between them and a novel \emph{zero capacity multi-critical point} where all three phases meet. For the $Z_2$ code, we provide two practical error correction procedures that fall short of the optimal codes and qualitatively alter the phase diagram, splitting the multi-critical point into two second-order coding no-coding phase transitions. Carrying out our approach on current noisy devices could provide a systematic way to construct quantum codes for robust computation and communication.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2503.15482v1" target="_blank">Natural Quantization of Neural Networks</a></h3>
            <p><strong>Authors:</strong> Richard Barney, Djamil Lakhdar-Hamina, Victor Galitski</p>
            <p>We propose a natural quantization of a standard neural network, where the neurons correspond to qubits and the activation functions are implemented via quantum gates and measurements. The simplest quantized neural network corresponds to applying single-qubit rotations, with the rotation angles being dependent on the weights and measurement outcomes of the previous layer. This realization has the advantage of being smoothly tunable from the purely classical limit with no quantum uncertainty (thereby reproducing the classical neural network exactly) to a quantum case, where superpositions introduce an intrinsic uncertainty in the network. We benchmark this architecture on a subset of the standard MNIST dataset and find a regime of "quantum advantage," where the validation error rate in the quantum realization is smaller than that in the classical model. We also consider another approach where quantumness is introduced via weak measurements of ancilla qubits entangled with the neuron qubits. This quantum neural network also allows for smooth tuning of the degree of quantumness by controlling an entanglement angle, $g$, with $g=\frac\pi 2$ replicating the classical regime. We find that validation error is also minimized within the quantum regime in this approach. We also observe a quantum transition, with sharp loss of the quantum network's ability to learn at a critical point $g_c$. The proposed quantum neural networks are readily realizable in present-day quantum computers on commercial datasets.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2503.15479v1" target="_blank">Deep Mantle-Atmosphere Coupling and Carbonaceous Bombardment: Options   for Biomolecule Formation on an Oxidized Early Earth</a></h3>
            <p><strong>Authors:</strong> Klaus Paschek, Thomas K. Henning, Karan Molaverdikhani, Yoshinori Miyazaki, Ben K. D. Pearce, Ralph E. Pudritz, Dmitry A. Semenov</p>
            <p>Understanding what environmental conditions prevailed on early Earth during the Hadean eon, and how this set the stage for the origins of life, remains a challenge. Geologic processes such as serpentinization and bombardment by chondritic material during the late veneer might have been very active, shaping an atmospheric composition reducing enough to allow efficient photochemical synthesis of HCN, one of the key precursors of prebiotic molecules. HCN can rain out and accumulate in warm little ponds (WLPs), forming prebiotic molecules such as nucleobases and the sugar ribose. These molecules could condense to nucleotides, the building blocks of RNA molecules, one of the ingredients of life. Here, we perform a systematic study of potential sources of reducing gases on Hadean Earth and calculate the concentrations of prebiotic molecules in WLPs based on a comprehensive geophysical and atmospheric model. We find that in a reduced H$_2$-dominated atmosphere, carbonaceous bombardment can produce enough HCN to reach maximum WLP concentrations of $\sim 1-10\,\mathrm{mM}$ for nucleobases and, in the absence of seepage, $\sim 10-100\,\mathrm{\mu M}$ for ribose. If the Hadean atmosphere was initially oxidized and CO$_2$-rich ($90\,\%$), we find serpentinization alone can reduce the atmosphere, resulting in WLP concentrations of an order of magnitude lower than the reducing carbonaceous bombardment case. In both cases, concentrations are sufficient for nucleotide synthesis, as shown in experimental studies. RNA could have appeared on Earth immediately after it became habitable (about $100\,\mathrm{Myr}$ after the Moon-forming impact), or it could have (re)appeared later at any time up to the beginning of the Archean.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2408.01241v3" target="_blank">Radio-frequency cascade readout and coherent exchange control of spin   qubits fabricated using a 300 mm wafer process</a></h3>
            <p><strong>Authors:</strong> Jacob F. Chittock-Wood, Ross C. C. Leon, Michael A. Fogarty, Tara Murphy, Sofia M. Patomäki, Giovanni A. Oakes, James Williams, Felix-Ekkehard von Horstig, Nathan Johnson, Julien Jussot, Stefan Kubicek, Bogdan Govoreanu, David F. Wise, M. Fernando Gonzalez-Zalba, John J. L. Morton</p>
            <p>Leveraging advanced semiconductor manufacturing promises potential for scaling up silicon-based quantum processors. In this context, control and readout of individual spin qubits have been shown on devices manufactured on 300 mm wafer metal-oxide-semiconductor (MOS) processes, yet quantum processors require two-qubit interactions to operate. Here, we use a 300 mm natural silicon MOS process customized for spin qubits and demonstrate coherent control of two electron spins using the spin-spin exchange interaction, which forms the basis for entangling gates such as $\sqrt{\text{SWAP}}$. We observe gate dephasing times of up to $T_2^{*}\approx500$ ns and a gate quality factor of $10$. For readout, we introduce a novel dispersive readout technique, the radio-frequency electron cascade, that simplifies the qubits unit cell while providing high gain. We achieve a $74\%$ singlet-triplet readout fidelity in $8~\mu$s integration time, the highest-performing dispersive readout demonstration on a planar MOS process. The combination of sensitive dispersive readout with industrial-grade manufacturing marks a crucial step towards the large-scale integration of silicon quantum processors.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2503.15473v1" target="_blank">Variational Quantum Annealing for Quantum Chemistry</a></h3>
            <p><strong>Authors:</strong> Ka-Wa Yip, Kübra Yeter-Aydeniz, Sijia S. Dong</p>
            <p>We introduce a variational quantum annealing (VarQA) algorithm for electronic structure theory, in which we use the quantum annealer as a sampler and prepare an ansatz state through its statistics. We also introduce a strategy called the "digitizer" for searching the space of variational parameters efficiently. We demonstrate the effectiveness of VarQA by evaluating the ground-state potential energy surface for molecules with up to $20$ spin orbitals as well as an excited-state potential energy surface. This approach resembles the workings of the quantum Boltzmann Machines (QBMs), but is generalized to handle distributions beyond the Boltzmann distribution. In VarQA, with the number of required logical qubits equal to the number of spin orbitals, a fully connected Ising Hamiltonian can be readily implemented in a large-scale quantum annealer as a scalable ansatz for electronic structure calculations.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2412.02348v2" target="_blank">The mass of the gluino-glue bound state in large-$N$ $\mathcal{N}=1$   Supersymmetric Yang-Mills theory</a></h3>
            <p><strong>Authors:</strong> Claudio Bonanno, Margarita García Pérez, Antonio González-Arroyo, Ken-Ichi Ishikawa, Masanori Okawa</p>
            <p>We provide a first-principles non-perturbative determination of the mass of the lightest gluino-gluon bound state (gluino-glue) in large-$N$ $\mathcal{N}=1$ Supersymmetric Yang--Mills theory by means of numerical Monte Carlo simulations of the lattice-discretized theory, and exploiting large-$N$ twisted volume reduction. Our large-$N$ determination is consistent with naive extrapolation of previously-known $\mathrm{SU}(2)$ and $\mathrm{SU}(3)$ results.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2410.11039v2" target="_blank">Squeezing via self-induced transparency in mercury-filled photonic   crystal fibers</a></h3>
            <p><strong>Authors:</strong> M. S. Najafabadi, J. F. Corney, L. L. Sánchez-Soto, N. Y. Joly, G. Leuchs</p>
            <p>We investigate the squeezing of ultrashort pulses using self-induced transparency in a mercury-filled hollow-core photonic crystal fiber. Our focus is on quadrature squeezing at low mercury vapor pressures, with atoms near resonance on the $^3{\rm D}_3 \to 6^3{\rm P}_2$ transition. We vary the atomic density, thus the gas pressure (from 2.72 to 15.7$\mu$bar), by adjusting the temperature (from 273~K to 303 ~K). Our results show that achieving squeezing at room temperature, considering both fermionic and bosonic mercury isotopes, requires ultrashort femtosecond pulses. We also determine the optimal detection length for squeezing at different pressures and temperatures.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2503.15468v1" target="_blank">No Flavor Anisotropy in the High-Energy Neutrino Sky Upholds Lorentz   Invariance</a></h3>
            <p><strong>Authors:</strong> Bernanda Telalovic, Mauricio Bustamante</p>
            <p>Discovering Lorentz-invariance violation (LIV) would upend the foundations of modern physics. Because LIV effects grow with energy, high-energy astrophysical neutrinos provide the most sensitive tests of Lorentz invariance in the neutrino sector. We examine an understudied yet phenomenologically rich LIV signature: compass asymmetries, where neutrinos of different flavors propagate preferentially along different directions. Using the directional flavor composition of high-energy astrophysical neutrinos, i.e., the abundances of $\nu_{e}$, $\nu_{\mu}$, and $\nu_{\tau}$ across the sky, we find no evidence of LIV-induced flavor anisotropy in 7.5 years of IceCube High-Energy Starting Events. Thus, we place upper limits on the values of hundreds of LIV parameters with operator dimensions 2-8, tightening existing limits by orders of magnitude and bounding hundreds of parameters for the first time.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2503.15466v1" target="_blank">Supercell environments using GridRad-Severe and the HRRR: Addressing   discrepancies between prior tornado datasets</a></h3>
            <p><strong>Authors:</strong> Brice Coffer, Matthew Parker, Michael Coniglio, Cameron Homeyer</p>
            <p>Storm-relative helicity (SRH) is an important ingredient in supercell development, as well as mesocyclone intensity, and is linked to tornadogenesis and tornado potential. Derived from the storm-relative wind profile, SRH is composed of both the vertical wind shear and storm-relative flow. Recent studies have come to conflicting findings regarding whether shallower or deeper layers of SRH have more skill in tornado forecasting. Possible causes of this discrepancy include the use of observed versus model-based proximity soundings, as well as whether the storm-relative wind profile is determined via observed versus estimated storm motions. This study uses a new dataset of objectively identified supercells, with observed storm motions, paired with high-resolution model analyses to address the discrepancies among prior studies. Unlike in previous model-based tornado environmental datasets, the present approach reveals substantive differences in storm-relative flow, vertical wind shear, and SRH within the low-to-mid-levels between nontornadic and tornadic supercells. Using observed storm motions for storm-relative variables further magnifies differences in the low-to-mid-level storm-relative winds between nontornadic and tornadic supercells, ultimately leading to deeper layers of SRH having more forecast skill than near-ground SRH. Thus, the combination of a higher-resolution model analyses, which better represents the near-storm environment, with observed storm motions appears to explain why many past tornado climatologies using model-based environmental analyses have failed to find significant differences in the storm-relative wind profile. These results help bridge the gap between previous studies that employed coarser model-based analyses with those that aggregated observed soundings from field projects.</p>
        </div>
        <hr>
        
        <div class="paper">
            <h3><a href="http://arxiv.org/pdf/2503.15464v1" target="_blank">The 200 Gbps Challenge: Imagining HL-LHC analysis facilities</a></h3>
            <p><strong>Authors:</strong> Alexander Held, Sam Albin, Garhan Attebury, Kenneth Bloom, Brian Bockelman, Lincoln Bryant, Kyungeon Choi, Kyle Cranmer, Peter Elmer, Matthew Feickert, Rob Gardner, Lindsey Gray, Fengping Hu, David Lange, Carl Lundstedt, Peter Onyisi, Jim Pivarski, Oksana Shadura, Nick Smith, John Thiltges, Ben Tovar, Ilija Vukotic, Gordon Watts, Derek Weitzel, Andrew Wightman</p>
            <p>The IRIS-HEP software institute, as a contributor to the broader HEP Python ecosystem, is developing scalable analysis infrastructure and software tools to address the upcoming HL-LHC computing challenges with new approaches and paradigms, driven by our vision of what HL-LHC analysis will require. The institute uses a "Grand Challenge" format, constructing a series of increasingly large, complex, and realistic exercises to show the vision of HL-LHC analysis. Recently, the focus has been demonstrating the IRIS-HEP analysis infrastructure at scale and evaluating technology readiness for production.   As a part of the Analysis Grand Challenge activities, the institute executed a "200 Gbps Challenge", aiming to show sustained data rates into the event processing of multiple analysis pipelines. The challenge integrated teams internal and external to the institute, including operations and facilities, analysis software tools, innovative data delivery and management services, and scalable analysis infrastructure. The challenge showcases the prototypes - including software, services, and facilities - built to process around 200 TB of data in both the CMS NanoAOD and ATLAS PHYSLITE data formats with test pipelines.   The teams were able to sustain the 200 Gbps target across multiple pipelines. The pipelines focusing on event rate were able to process at over 30 MHz. These target rates are demanding; the activity revealed considerations for future testing at this scale and changes necessary for physicists to work at this scale in the future. The 200 Gbps Challenge has established a baseline on today's facilities, setting the stage for the next exercise at twice the scale.</p>
        </div>
        <hr>
        
<!-- END PAPERS -->
            </div>
        </section>
    </main>
</body>
</html>
